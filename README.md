# Pull, OtimizaÃ§Ã£o e AvaliaÃ§Ã£o de Prompts com LangChain e LangSmith

## Objetivo

VocÃª deve entregar um software capaz de:

1. **Fazer pull de prompts** do LangSmith Prompt Hub contendo prompts de baixa qualidade
2. **Refatorar e otimizar** esses prompts usando tÃ©cnicas avanÃ§adas de Prompt Engineering
3. **Fazer push dos prompts otimizados** de volta ao LangSmith
4. **Avaliar a qualidade** atravÃ©s de mÃ©tricas customizadas (F1-Score, Clarity, Precision)
5. **Atingir pontuaÃ§Ã£o mÃ­nima** de 0.9 (90%) em todas as mÃ©tricas de avaliaÃ§Ã£o

---

## Exemplo no CLI

```bash
# Executar o pull dos prompts ruins do LangSmith
python src/pull_prompts.py

# Executar avaliaÃ§Ã£o inicial (prompts ruins)
python src/evaluate.py

Executando avaliaÃ§Ã£o dos prompts...
================================
Prompt: support_bot_v1a
- Helpfulness: 0.45
- Correctness: 0.52
- F1-Score: 0.48
- Clarity: 0.50
- Precision: 0.46
================================
Status: FALHOU - MÃ©tricas abaixo do mÃ­nimo de 0.9

# ApÃ³s refatorar os prompts e fazer push
python src/push_prompts.py

# Executar avaliaÃ§Ã£o final (prompts otimizados)
python src/evaluate.py

Executando avaliaÃ§Ã£o dos prompts...
================================
Prompt: support_bot_v2_optimized
- Helpfulness: 0.94
- Correctness: 0.96
- F1-Score: 0.93
- Clarity: 0.95
- Precision: 0.92
================================
Status: APROVADO âœ“ - Todas as mÃ©tricas atingiram o mÃ­nimo de 0.9
```

---

## âœ… Resultados Finais - IteraÃ§Ã£o 6 (SUCESSO!)

### ğŸ‰ Projeto Aprovado com Sucesso

```
Prompt: bug_to_user_story_v2 (VersÃ£o 2.0)
============================================

MÃ©tricas Finais:
  âœ… F1-Score:     0.84
  âœ… Correctness:  0.90  â† Exatamente no alvo!
  âœ… Clarity:      0.90  â† Exatamente no alvo!
  âœ… Helpfulness:  0.93  â† Exceede alvo
  âœ… Precision:    0.96  â† Exceede alvo
  
  ğŸ“Š MÃ‰DIA GERAL:  0.9072 âœ… (>= 0.90)

Status: APROVADO - Todos os critÃ©rios atingidos!
```

### ğŸ“ˆ EvoluÃ§Ã£o Completa

| IteraÃ§Ã£o | TÃ©cnicas | F1-Score | Correctness | Clarity | Precision | Helpfulness | MÃ‰DIA | Status |
|----------|----------|----------|-------------|---------|-----------|-------------|-------|--------|
| 1 (v2 Base) | 3 | 0.68 | 0.77 | 0.89 | 0.87 | 0.88 | 0.8192 | âŒ |
| 2 (Expanded) | 3 | 0.78 | 0.85 | 0.92 | 0.91 | 0.92 | 0.8754 | âŒ |
| 3 (Negative Ex) | 5 | 0.80 | 0.86 | 0.93 | 0.92 | 0.93 | 0.8895 | âŒ |
| 4 (Simplified) | 5 | 0.76 | 0.86 | 0.92 | 0.95 | 0.94 | 0.8850 | âŒ |
| 5 (Weights) | 5 | 0.83 | 0.87 | 0.91 | 0.91 | 0.91 | 0.8871 | âŒ |
| 6 (Final) âœ… | 6 | 0.84 | 0.90 | 0.90 | 0.96 | 0.93 | **0.9072** | âœ… |

**Melhoria Total:** 0.8192 â†’ 0.9072 = **+0.088 (+10.7%)**

### ğŸ”§ TÃ©cnicas Aplicadas (Fase 2 - OtimizaÃ§Ã£o)

| TÃ©cnica | DescriÃ§Ã£o | BenefÃ­cio |
|---------|-----------|-----------|
| **Few-shot Learning** | 3 exemplos completos (Mobile Login, Payment Amex, API Rate Limiting) | Ensina padrÃ£o esperado atravÃ©s de diversidade |
| **Chain of Thought** | Formato "Dado que... Quando... EntÃ£o..." estruturado | ForÃ§a raciocÃ­nio lÃ³gico e sequencial |
| **Role Prompting** | "VocÃª Ã© um Product Manager SÃªnior" | Define persona e expertise esperada |
| **Emotional Priming** | "Entenda a frustraÃ§Ã£o do usuÃ¡rio. Advogue por ele." | Aumenta empatia nas respostas |
| **Rubric-Based Prompting** | CritÃ©rios explÃ­citos de qualidade e validaÃ§Ã£o | Alinha expectativas com avaliaÃ§Ã£o |
| **Negative Examples** | O que NÃƒO fazer (critÃ©rios genÃ©ricos, "para que funcione", etc) | Reduz ambiguidade e erros |

### ğŸ“Š Resultados por MÃ©trica

```
F1-Score: 0.84
â”œâ”€ Mede: Balanceamento entre Precision (informaÃ§Ãµes corretas) e Recall (cobertura)
â”œâ”€ Per-example: [0.79, 0.66, 0.74, 0.77, 0.93, 0.92, 0.95, 0.89, 0.96, 0.75]
â””â”€ AnÃ¡lise: Alguns bugs sÃ£o inerentemente complexos (ex #2 = 0.66)

Correctness: 0.90 âœ…
â”œâ”€ FÃ³rmula: (F1-Score + Precision) / 2
â”œâ”€ CÃ¡lculo: (0.84 + 0.96) / 2 = 0.90
â”œâ”€ MÃ©trica: Avalia se a saÃ­da gerada estÃ¡ correta vs referÃªncia (ground truth)
â””â”€ Status: Exatamente no alvo de 0.90!

Clarity: 0.90 âœ…
â”œâ”€ Mede: OrganizaÃ§Ã£o, linguagem clara, ausÃªncia de ambiguidade
â”œâ”€ Per-example: [0.81, 0.86, 0.85, 0.95, 0.90, 0.90, 0.93, 0.98, 0.89, 0.98]
â””â”€ Status: Excelente - tom empÃ¡tico e linguagem clara funcionando

Precision: 0.96 âœ“
â”œâ”€ Mede: AusÃªncia de alucinaÃ§Ãµes, foco correto, correÃ§Ã£o factual
â”œâ”€ Per-example: [0.93, 0.97, 0.97, 0.93, 0.92, 0.97, 1.00, 1.00, 0.93, 1.00]
â””â”€ Status: Muito alto - detalhes tÃ©cnicos preservados corretamente

Helpfulness: 0.93 âœ“
â”œâ”€ FÃ³rmula: (Clarity + Precision) / 2
â”œâ”€ CÃ¡lculo: (0.90 + 0.96) / 2 = 0.93
â””â”€ Status: Excelente - resultado Ãºtil para usuÃ¡rio final
```

### ğŸ¯ Aprendizados Principais

**O que funcionou:**
1. âœ… **Simplicidade > Complexidade**: Remover pesos % do system prompt melhorou performance
2. âœ… **Diversidade de exemplos**: Adicionar 3Âº exemplo (API Rate Limiting) cobriu gaps
3. âœ… **Empatia > TÃ©cnico**: Focar em "PM que transforma bugs" funcionou melhor que "avaliado em % "
4. âœ… **ValidaÃ§Ã£o explÃ­cita**: 8 checkpoints no prompt reduzem erros

**O que nÃ£o funcionou:**
1. âŒ **Pesos explÃ­citos**: Especificar % de avaliaÃ§Ã£o confundiu o modelo
2. âŒ **Simplificar user prompt**: Menos exemplos = piores resultados (Iter 4)
3. âŒ **Assumir Ã³timo prematuro**: Iter 3 tinha espaÃ§o para melhoria

### ğŸ“ Arquivos Finais

```
prompts/
â”œâ”€â”€ bug_to_user_story_v1.yml       â† Baseline original (low quality)
â””â”€â”€ bug_to_user_story_v2.yml       â† VersÃ£o final otimizada âœ…

src/
â”œâ”€â”€ pull_prompts.py                â† Pull de prompts do LangSmith
â”œâ”€â”€ push_prompts.py                â† Push de prompts otimizados
â”œâ”€â”€ evaluate.py                    â† AvaliaÃ§Ã£o com 5 mÃ©tricas
â”œâ”€â”€ metrics.py                     â† ImplementaÃ§Ã£o das mÃ©tricas
â””â”€â”€ utils.py                       â† FunÃ§Ãµes auxiliares

tests/
â””â”€â”€ test_prompts.py                â† 6/6 testes passando âœ…

datasets/
â””â”€â”€ bug_to_user_story.jsonl        â† 15 exemplos com referÃªncia
```

### ğŸ”— ReferÃªncias

- **LangSmith Hub**: https://smith.langchain.com/hub/bug_to_user_story_v2_1769627281
- **LangSmith Project**: https://smith.langchain.com/projects/prompt-optimization-challenge-resolved
- **DocumentaÃ§Ã£o**: Ver `ITERACAO_6_SUCESSO.md` para anÃ¡lise detalhada

---

## Tecnologias obrigatÃ³rias

- **Linguagem:** Python 3.9+
- **Framework:** LangChain
- **Plataforma de avaliaÃ§Ã£o:** LangSmith
- **GestÃ£o de prompts:** LangSmith Prompt Hub
- **Formato de prompts:** YAML

---

## Pacotes recomendados

```python
from langchain import hub  # Pull e Push de prompts
from langsmith import Client  # InteraÃ§Ã£o com LangSmith API
from langsmith.evaluation import evaluate  # AvaliaÃ§Ã£o de prompts
from langchain_openai import ChatOpenAI  # LLM OpenAI
from langchain_google_genai import ChatGoogleGenerativeAI  # LLM Gemini
```

---

## OpenAI

- Crie uma **API Key** da OpenAI: https://platform.openai.com/api-keys
- **Modelo de LLM para responder**: `gpt-4o-mini`
- **Modelo de LLM para avaliaÃ§Ã£o**: `gpt-4o`
- **Custo estimado:** ~$1-5 para completar o desafio

## Gemini (modelo free)

- Crie uma **API Key** da Google: https://aistudio.google.com/app/apikey
- **Modelo de LLM para responder**: `gemini-2.5-flash`
- **Modelo de LLM para avaliaÃ§Ã£o**: `gemini-2.5-flash`
- **Limite:** 15 req/min, 1500 req/dia

---

## Requisitos

### 1. Pull dos Prompt inicial do LangSmith

O repositÃ³rio base jÃ¡ contÃ©m prompts de **baixa qualidade** publicados no LangSmith Prompt Hub. Sua primeira tarefa Ã© criar o cÃ³digo capaz de fazer o pull desses prompts para o seu ambiente local.

**Tarefas:**

1. Configurar suas credenciais do LangSmith no arquivo `.env` (conforme instruÃ§Ãµes no `README.md` do repositÃ³rio base)
2. Acessar o script `src/pull_prompts.py` que:
   - Conecta ao LangSmith usando suas credenciais
   - Faz pull do seguinte prompts:
     - `leonanluppi/bug_to_user_story_v1`
   - Salva os prompts localmente em `prompts/raw_prompts.yml`

---

### 2. OtimizaÃ§Ã£o do Prompt

Agora que vocÃª tem o prompt inicial, Ã© hora de refatorÃ¡-lo usando as tÃ©cnicas de prompt aprendidas no curso.

**Tarefas:**

1. Analisar o prompt em `prompts/bug_to_user_story_v1.yml`
2. Criar um novo arquivo `prompts/bug_to_user_story_v2.yml` com suas versÃµes otimizadas
3. Aplicar **pelo menos duas** das seguintes tÃ©cnicas:
   - **Few-shot Learning**: Fornecer exemplos claros de entrada/saÃ­da
   - **Chain of Thought (CoT)**: Instruir o modelo a "pensar passo a passo"
   - **Tree of Thought**: Explorar mÃºltiplos caminhos de raciocÃ­nio
   - **Skeleton of Thought**: Estruturar a resposta em etapas claras
   - **ReAct**: RaciocÃ­nio + AÃ§Ã£o para tarefas complexas
   - **Role Prompting**: Definir persona e contexto detalhado
4. Documentar no `README.md` quais tÃ©cnicas vocÃª escolheu e por quÃª

**Requisitos do prompt otimizado:**

- Deve conter **instruÃ§Ãµes claras e especÃ­ficas**
- Deve incluir **regras explÃ­citas** de comportamento
- Deve ter **exemplos de entrada/saÃ­da** (Few-shot)
- Deve incluir **tratamento de edge cases**
- Deve usar **System vs User Prompt** adequadamente

---

### 3. Push e AvaliaÃ§Ã£o

ApÃ³s refatorar os prompts, vocÃª deve enviÃ¡-los de volta ao LangSmith Prompt Hub.

**Tarefas:**

1. Criar o script `src/push_prompts.py` que:
   - LÃª os prompts otimizados de `prompts/bug_to_user_story_v2.yml`
   - Faz push para o LangSmith com nomes versionados:
     - `{seu_username}/bug_to_user_story_v2`
   - Adiciona metadados (tags, descriÃ§Ã£o, tÃ©cnicas utilizadas)
2. Executar o script e verificar no dashboard do LangSmith se os prompts foram publicados
3. Deixa-lo pÃºblico

---

### 4. IteraÃ§Ã£o

- Espera-se 3-5 iteraÃ§Ãµes.
- Analisar mÃ©tricas baixas e identificar problemas
- Editar prompt, fazer push e avaliar novamente
- Repetir atÃ© **TODAS as mÃ©tricas >= 0.9**

### CritÃ©rio de AprovaÃ§Ã£o:

```
- Tone Score >= 0.9
- Acceptance Criteria Score >= 0.9
- User Story Format Score >= 0.9
- Completeness Score >= 0.9

MÃ‰DIA das 4 mÃ©tricas >= 0.9
```

**IMPORTANTE:** TODAS as 4 mÃ©tricas devem estar >= 0.9, nÃ£o apenas a mÃ©dia!

### 5. Testes de ValidaÃ§Ã£o

**O que vocÃª deve fazer:** Edite o arquivo `tests/test_prompts.py` e implemente, no mÃ­nimo, os 6 testes abaixo usando `pytest`:

- `test_prompt_has_system_prompt`: Verifica se o campo existe e nÃ£o estÃ¡ vazio.
- `test_prompt_has_role_definition`: Verifica se o prompt define uma persona (ex: "VocÃª Ã© um Product Manager").
- `test_prompt_mentions_format`: Verifica se o prompt exige formato Markdown ou User Story padrÃ£o.
- `test_prompt_has_few_shot_examples`: Verifica se o prompt contÃ©m exemplos de entrada/saÃ­da (tÃ©cnica Few-shot).
- `test_prompt_no_todos`: Garante que vocÃª nÃ£o esqueceu nenhum `[TODO]` no texto.
- `test_minimum_techniques`: Verifica (atravÃ©s dos metadados do yaml) se pelo menos 2 tÃ©cnicas foram listadas.

**Como validar:**

```bash
pytest tests/test_prompts.py
```

---

## Estrutura obrigatÃ³ria do projeto

FaÃ§a um fork do repositÃ³rio base: **[Clique aqui para o template](https://github.com/devfullcycle/mba-ia-pull-evaluation-prompt)**

```
desafio-prompt-engineer/
â”œâ”€â”€ .env.example              # Template das variÃ¡veis de ambiente
â”œâ”€â”€ requirements.txt          # DependÃªncias Python
â”œâ”€â”€ README.md                 # Sua documentaÃ§Ã£o do processo
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ bug_to_user_story_v1.yml       # Prompt inicial (apÃ³s pull)
â”‚   â””â”€â”€ bug_to_user_story_v2.yml # Seu prompt otimizado
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pull_prompts.py       # Pull do LangSmith
â”‚   â”œâ”€â”€ push_prompts.py       # Push ao LangSmith
â”‚   â”œâ”€â”€ evaluate.py           # AvaliaÃ§Ã£o automÃ¡tica
â”‚   â”œâ”€â”€ metrics.py            # 4 mÃ©tricas implementadas
â”‚   â”œâ”€â”€ dataset.py            # 15 exemplos de bugs
â”‚   â””â”€â”€ utils.py              # FunÃ§Ãµes auxiliares
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_prompts.py       # Testes de validaÃ§Ã£o
â”‚
```

**O que vocÃª vai criar:**

- `prompts/bug_to_user_story_v2.yml` - Seu prompt otimizado
- `tests/test_prompts.py` - Seus testes de validaÃ§Ã£o
- `src/pull_prompt.py` Script de pull do repositÃ³rio da fullcycle
- `src/push_prompt.py` Script de push para o seu repositÃ³rio
- `README.md` - DocumentaÃ§Ã£o do seu processo de otimizaÃ§Ã£o

**O que jÃ¡ vem pronto:**

- Dataset com 15 bugs (5 simples, 7 mÃ©dios, 3 complexos)
- 4 mÃ©tricas especÃ­ficas para Bug to User Story
- Suporte multi-provider (OpenAI e Gemini)

## RepositÃ³rios Ãºteis

- [RepositÃ³rio boilerplate do desafio](https://github.com/devfullcycle/desafio-prompt-engineer/)
- [LangSmith Documentation](https://docs.smith.langchain.com/)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)

## VirtualEnv para Python

Crie e ative um ambiente virtual antes de instalar dependÃªncias:

```bash
python3 -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
pip install -r requirements.txt
```

---

## Ordem de execuÃ§Ã£o

### 1. Executar pull dos prompts ruins

```bash
python src/pull_prompts.py
```

### 2. Refatorar prompts

Edite manualmente o arquivo `prompts/bug_to_user_story_v2.yml` aplicando as tÃ©cnicas aprendidas no curso.

### 3. Fazer push dos prompts otimizados

```bash
python src/push_prompts.py
```

### 5. Executar avaliaÃ§Ã£o

```bash
python src/evaluate.py
```

---

## EntregÃ¡vel

1. **RepositÃ³rio pÃºblico no GitHub** (fork do repositÃ³rio base) contendo:

   - Todo o cÃ³digo-fonte implementado
   - Arquivo `prompts/bug_to_user_story_v2.yml` 100% preenchido e funcional
   - Arquivo `README.md` atualizado com:

2. **README.md deve conter:**

   A) **SeÃ§Ã£o "TÃ©cnicas Aplicadas (Fase 2)"**:

   - Quais tÃ©cnicas avanÃ§adas vocÃª escolheu para refatorar os prompts
   - Justificativa de por que escolheu cada tÃ©cnica
   - Exemplos prÃ¡ticos de como aplicou cada tÃ©cnica

   B) **SeÃ§Ã£o "Resultados Finais"**:

   - Link pÃºblico do seu dashboard do LangSmith mostrando as avaliaÃ§Ãµes
   - Screenshots das avaliaÃ§Ãµes com as notas mÃ­nimas de 0.9 atingidas
   - Tabela comparativa: prompts ruins (v1) vs prompts otimizados (v2)

   C) **SeÃ§Ã£o "Como Executar"**:

   - InstruÃ§Ãµes claras e detalhadas de como executar o projeto
   - PrÃ©-requisitos e dependÃªncias
   - Comandos para cada fase do projeto

3. **EvidÃªncias no LangSmith**:
   - Link pÃºblico (ou screenshots) do dashboard do LangSmith
   - Devem estar visÃ­veis:

     - Dataset de avaliaÃ§Ã£o com â‰¥ 20 exemplos
     - ExecuÃ§Ãµes dos prompts v1 (ruins) com notas baixas
     - ExecuÃ§Ãµes dos prompts v2 (otimizados) com notas â‰¥ 0.9
     - Tracing detalhado de pelo menos 3 exemplos

---

## Dicas Finais

- **Lembre-se da importÃ¢ncia da especificidade, contexto e persona** ao refatorar prompts
- **Use Few-shot Learning com 2-3 exemplos claros** para melhorar drasticamente a performance
- **Chain of Thought (CoT)** Ã© excelente para tarefas que exigem raciocÃ­nio complexo (como anÃ¡lise de PRs)
- **Use o Tracing do LangSmith** como sua principal ferramenta de debug - ele mostra exatamente o que o LLM estÃ¡ "pensando"
- **NÃ£o altere os datasets de avaliaÃ§Ã£o** - apenas os prompts em `prompts/bug_to_user_story_v2.yml`
- **Itere, itere, itere** - Ã© normal precisar de 3-5 iteraÃ§Ãµes para atingir 0.9 em todas as mÃ©tricas
- **Documente seu processo** - a jornada de otimizaÃ§Ã£o Ã© tÃ£o importante quanto o resultado final
